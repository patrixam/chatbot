{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQSHpf8KCNemtx2+rxmHsM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrixam/chatbot/blob/main/probando/Preprocesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza Texto"
      ],
      "metadata": {
        "id": "9BNDjJYrhiiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQwTFYE3hVj3",
        "outputId": "3ff9790d-a994-4afe-94f6-805182395b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola visita  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r\"http\\S+|www\\S+\", \"\", texto)  # Eliminar URLs\n",
        "    texto = re.sub(r\"[^a-zA-Z\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
        "    return texto.lower()  # Convertir a minﾃｺsculas\n",
        "\n",
        "print(limpiar_texto(\"ﾂ｡Hola! Visita https://example.com 沁噂"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizacion"
      ],
      "metadata": {
        "id": "AqiHbC1YvZL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spacy"
      ],
      "metadata": {
        "id": "Mpau8fbrtr9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "0ntWOCcN5ltM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#descargar es_core_news_sm\n",
        "!spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "AJhFjiwB5XU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "texto = \"Hola, ﾂｿcﾃｳmo estﾃ｡s?\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhBuAoCnh9HH",
        "outputId": "c05928c5-9cc4-42fe-b422-8852db4cae76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', 'ﾂｿ', 'cﾃｳmo', 'estﾃ｡s', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nltk"
      ],
      "metadata": {
        "id": "pBwdIbdTtumw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASpDfsaR6gzS",
        "outputId": "2e008134-cc38-4244-c4ad-fb06c421b98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')  # Descargar datos necesarios para tokenizaciﾃｳn\n",
        "\n",
        "texto = \"Hola, ﾂｿcﾃｳmo estﾃ｡s?\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU2Rxrtt5FN3",
        "outputId": "a85cb936-5de2-42ac-abab-1dc598b0089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', 'ﾂｿcﾃｳmo', 'estﾃ｡s', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "poIYwp3Qt3yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spacy"
      ],
      "metadata": {
        "id": "q8qrAVrb7bxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina stopwords\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "texto = \"ﾂ｡Hola a todos! ﾂｿCuﾃ｡l es la direcciﾃｳn del sitio web www.example.com? Gracias.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "tokens = [token.text for token in doc if not token.is_stop]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qAnVDW-4v2J",
        "outputId": "853890bc-2129-484b-e718-e6a391a3d6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ﾂ｡', 'Hola', '!', 'ﾂｿ', 'direcciﾃｳn', 'sitio', 'web', 'www.example.com', '?', 'Gracias', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nltk"
      ],
      "metadata": {
        "id": "OTNZdwV_7Zni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina stopswords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Descargar datos necesarios para tokenizaciﾃｳn\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texto = \"ﾂ｡Hola a todos! ﾂｿCuﾃ｡l es la direcciﾃｳn del sitio web www.example.com? Gracias.\"\n",
        "tokens = word_tokenize(texto)\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX8ImfOh69Q4",
        "outputId": "0fa4c545-fc01-4925-bb0e-43a86f08b333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ﾂ｡Hola', '!', 'ﾂｿCuﾃ｡l', 'direcciﾃｳn', 'sitio', 'web', 'www.example.com', '?', 'Gracias', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalizaciﾃｳn"
      ],
      "metadata": {
        "id": "geefRIxLt6UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "yNL_Lbbdvk36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')  # Descargar datos necesarios para tokenizaciﾃｳn\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Inicializar el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"Corriendo, corrﾃｭ rﾃ｡pidamente y los corredores corrﾃｭan.\"\n",
        "\n",
        "# Tokenizar el texto\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "# Aplicar stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Palabras originales:\", tokens)\n",
        "print(\"Raﾃｭces (stems):\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6MN5X9giHRx",
        "outputId": "2318a687-e1cf-4680-afd8-36c69da35e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras originales: ['Corriendo', ',', 'corrﾃｭ', 'rﾃ｡pidamente', 'y', 'los', 'corredores', 'corrﾃｭan', '.']\n",
            "Raﾃｭces (stems): ['corriendo', ',', 'corrﾃｭ', 'rﾃ｡pidament', 'y', 'lo', 'corredor', 'corrﾃｭan', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lematizacion"
      ],
      "metadata": {
        "id": "4hJoq2liwfAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"Corriendo, corrﾃｭ rﾃ｡pidamente y los corredores corrﾃｭan.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Aplicar lematizacion\n",
        "lemas = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Palabras originales:\", tokens)\n",
        "print(\"Lemas:\", lemas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vV04I4Ywgc6",
        "outputId": "a1a633aa-77e4-488e-df71-65d07058d78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras originales: ['ﾂ｡Hola', 'a', 'todos', '!', 'ﾂｿCuﾃ｡l', 'es', 'la', 'direcciﾃｳn', 'del', 'sitio', 'web', 'www.example.com', '?', 'Gracias', '.']\n",
            "Lemas: ['correr', ',', 'corrﾃｭ', 'rﾃ｡pidamente', 'y', 'el', 'corredor', 'correr', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejemplo Completo"
      ],
      "metadata": {
        "id": "ltAhIwCRwRA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Corpus de ejemplo\n",
        "corpus = [\n",
        "    \"ﾂ｡Hola! ﾂｿCﾃｳmo estﾃ｡s? Visita https://example.com para mﾃ｡s informaciﾃｳn.\",\n",
        "    \"Me encanta este curso de PLN.\",\n",
        "    \"ﾂｿDﾃｳnde estﾃ｡n las oficinas?\",\n",
        "    \"En el 2024, implementaremos mﾃ｡s IA.\"\n",
        "]\n",
        "\n",
        "# Pipeline completo\n",
        "def preprocesar(texto):\n",
        "    # Limpieza\n",
        "    texto = re.sub(r\"http\\S+|www\\S+\", \"\", texto)  # Eliminar URLs\n",
        "    texto = re.sub(r\"\\d+\", \"\", texto)  # Eliminar nﾃｺmeros\n",
        "    texto = re.sub(r\"[^a-zA-Zﾃ｡ﾃｩﾃｭﾃｳﾃｺﾃｱﾃ曾\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
        "    texto = texto.lower()  # Minﾃｺsculas\n",
        "\n",
        "    # Tokenizaciﾃｳn\n",
        "    tokens = word_tokenize(texto)\n",
        "\n",
        "    # Stemming\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    stems = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematizaciﾃｳn\n",
        "    doc = nlp(texto)\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "    return {\"original\": texto, \"tokens\": tokens, \"stems\": stems, \"lemas\": lemas}\n",
        "\n",
        "# Aplicar pipeline al corpus\n",
        "resultados = [preprocesar(texto) for texto in corpus]\n",
        "for i, res in enumerate(resultados):\n",
        "    print(f\"Texto {i+1}:\")\n",
        "    print(\"Original:\", res[\"original\"])\n",
        "    print(\"Tokens:\", res[\"tokens\"])\n",
        "    print(\"Stems:\", res[\"stems\"])\n",
        "    print(\"Lemas:\", res[\"lemas\"])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RbNCAaliLLS",
        "outputId": "d4a390d8-389d-4fa0-f386-45624311de94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1:\n",
            "Original: hola cﾃｳmo estﾃ｡s visita  para mﾃ｡s informaciﾃｳn\n",
            "Tokens: ['hola', 'cﾃｳmo', 'estﾃ｡s', 'visita', 'para', 'mﾃ｡s', 'informaciﾃｳn']\n",
            "Stems: ['hola', 'cﾃｳmo', 'visita', 'informaciﾃｳn']\n",
            "Lemas: ['holar', 'estar', 'visita', ' ', 'informaciﾃｳn']\n",
            "--------------------------------------------------\n",
            "Texto 2:\n",
            "Original: me encanta este curso de pln\n",
            "Tokens: ['me', 'encanta', 'este', 'curso', 'de', 'pln']\n",
            "Stems: ['encanta', 'curso', 'pln']\n",
            "Lemas: ['encantar', 'curso', 'pln']\n",
            "--------------------------------------------------\n",
            "Texto 3:\n",
            "Original: dﾃｳnde estﾃ｡n las oficinas\n",
            "Tokens: ['dﾃｳnde', 'estﾃ｡n', 'las', 'oficinas']\n",
            "Stems: ['dﾃｳnde', 'oficina']\n",
            "Lemas: ['oficina']\n",
            "--------------------------------------------------\n",
            "Texto 4:\n",
            "Original: en el  implementaremos mﾃ｡s ia\n",
            "Tokens: ['en', 'el', 'implementaremos', 'mﾃ｡s', 'ia']\n",
            "Stems: ['implementaremo', 'ia']\n",
            "Lemas: [' ', 'implementaremo', 'ia']\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corregir Ortografﾃｭa"
      ],
      "metadata": {
        "id": "iXmduvj_nMpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TextBlob"
      ],
      "metadata": {
        "id": "PhXu3h8InWtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "texto = \"Corrienddo, corﾃｭ rapdamente y los corredoress corian.\"\n",
        "blob = TextBlob(texto)\n",
        "\n",
        "# Correcciﾃｳn ortogrﾃ｡fica\n",
        "texto_corregido = str(blob.correct())\n",
        "print(\"Texto corregido:\", texto_corregido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxe2t2tknMSt",
        "outputId": "bd391a83-cb3a-4e95-c7f9-a169574590f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: Corrienddo, cord rapdamente y los corredoress corn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SymSpell"
      ],
      "metadata": {
        "id": "0UJFXlXynueT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy"
      ],
      "metadata": {
        "id": "ZRUZp5Stn0X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "# Inicializar SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
        "sym_spell.load_dictionary(\"frequency_dictionary_es.txt\", term_index=0, count_index=1)\n",
        "\n",
        "# Corregir texto\n",
        "texto = \"corrienddo rapdamente los corredoress corian\"\n",
        "correcciones = [sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)[0].term\n",
        "                if sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2) else word\n",
        "                for word in texto.split()]\n",
        "print(\"Texto corregido:\", \" \".join(correcciones))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxUvMQ8xnxpF",
        "outputId": "d69406b3-e68d-42a9-b515-79a65239ffa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: ['corrienddo', 'rapdamente', 'los', 'corredoress', 'corian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hunspell"
      ],
      "metadata": {
        "id": "WzFHyNl_oJKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instalar hunspell\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install libhunspell-dev hunspell\n",
        "!pip install hunspell\n"
      ],
      "metadata": {
        "id": "LxTbS-5EoMSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instalar diccionario\n",
        "!sudo apt-get install hunspell-es\n"
      ],
      "metadata": {
        "id": "JLv-4nwyr91Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hunspell\n",
        "\n",
        "# Inicializa Hunspell con el diccionario espaﾃｱol\n",
        "hspell = hunspell.HunSpell('/usr/share/hunspell/es_ES.dic', '/usr/share/hunspell/es_ES.aff')\n",
        "\n",
        "# Texto original\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto.split()\n",
        "\n",
        "# Corregir cada palabra\n",
        "correcciones = [\n",
        "    hspell.suggest(palabra)[0] if not hspell.spell(palabra) and hspell.suggest(palabra) else palabra\n",
        "    for palabra in palabras\n",
        "]\n",
        "\n",
        "# Texto corregido\n",
        "texto_corregido = \" \".join(correcciones)\n",
        "print(\"Texto corregido:\", texto_corregido)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBoP-TWkskrJ",
        "outputId": "14df74db-8660-44ee-9e97-969555431065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rﾃ｡pidamente los corredores coran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pyspellchecker"
      ],
      "metadata": {
        "id": "TomHxvRCoa2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCGUdIuBoiB2",
        "outputId": "2023ceeb-b45c-4b73-fb35-248d784dae4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.2/7.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m2.2/7.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.0/7.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Inicializar para espaﾃｱol\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "# Palabras a corregir\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "tokens = texto.split()\n",
        "\n",
        "# Correcciﾃｳn ortogrﾃ｡fica\n",
        "correcciones = [spell.correction(token) for token in tokens]\n",
        "print(\"Texto corregido:\", correcciones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbf31Wpod73",
        "outputId": "7a00355c-0175-4c8c-87a8-92946b86026c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: ['correinado', 'rﾃ｡pidamente', 'los', None, 'coria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener sugerencias para una palabra incorrecta\n",
        "palabra = \"corrienndo\"\n",
        "sugerencias = spell.candidates(palabra)\n",
        "print(f\"Sugerencias para '{palabra}':\", sugerencias)\n",
        "print(f\"Sugerencias para 'corredoress':\", spell.candidates('corredoress'))\n",
        "print(f\"Sugerencias para 'corian':\", spell.candidates('corian'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7IiWkTko7B6",
        "outputId": "da2c3c72-09f4-45cf-97b5-20cccf32dfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugerencias para 'corrienndo': {'corrigendo', 'correinado'}\n",
            "Sugerencias para 'corredoress': None\n",
            "Sugerencias para 'corian': {'coriana', 'coriano', 'coria', 'corion'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar palabras personalizadas al diccionario\n",
        "spell.word_frequency.load_words(['corriendo', 'corrﾃｭan', 'corredores'])\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "correcciones = [spell.correction(token) for token in texto.split()]\n",
        "print(\"Texto corregido:\", \" \".join(correcciones))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FfbtesRo8nC",
        "outputId": "41549676-ceba-452e-f992-9f092d4906b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rﾃ｡pidamente los corredores coria\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uso de diccionarios (difflib)"
      ],
      "metadata": {
        "id": "ZTDF6ZvundOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "diccionario = [\"corriendo\", \"corrﾃｭ\", \"rﾃ｡pidamente\", \"corredores\", \"corrﾃｭan\"]\n",
        "texto = \"Corrienddo corﾃｭ rapdamente los corredoress corian.\"\n",
        "tokens = texto.lower().split()\n",
        "\n",
        "# Buscar la palabra mﾃ｡s cercana en el diccionario\n",
        "corregido = [difflib.get_close_matches(token, diccionario, n=1, cutoff=0.8)[0]\n",
        "             if difflib.get_close_matches(token, diccionario, n=1, cutoff=0.8) else token\n",
        "             for token in tokens]\n",
        "\n",
        "print(\"Texto corregido:\", \" \".join(corregido))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqtpq3JUnho7",
        "outputId": "840f8085-c65f-4713-a51d-3e08f57f41a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo corrﾃｭ rﾃ｡pidamente los corredores corian.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Emojis\n",
        "Convertir Emojis a Texto:\n",
        "En anﾃ｡lisis de sentimientos o redes sociales, los emojis pueden expresar emociones.\n"
      ],
      "metadata": {
        "id": "i8DvrLcCWAzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fgwoBciWJhL",
        "outputId": "120fb436-7aec-4269-f7c9-7d717a527bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m235.5/590.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "texto = \"ﾂ｡Este producto es increﾃｭble! 沽昨汨構"\n",
        "texto_convertido = emoji.demojize(texto)\n",
        "print(\"Texto con emojis convertidos:\", texto_convertido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZLQ3lQGWGbG",
        "outputId": "6388ee1a-cf07-4b05-affd-7305fbcec3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto con emojis convertidos: ﾂ｡Este producto es increﾃｭble! :smiling_face_with_heart-eyes::thumbs_up:\n"
          ]
        }
      ]
    }
  ]
}